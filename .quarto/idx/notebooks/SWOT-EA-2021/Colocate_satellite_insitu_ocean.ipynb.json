{"title":"<span style=\"color:orange\">Use Case: Co-locate satellite and in-situ data for cross-validation</span>","markdown":{"headingText":"<span style=\"color:orange\">Use Case: Co-locate satellite and in-situ data for cross-validation</span>","containsRefs":false,"markdown":"\n\n#### User Story\nAs a coastal applications researcher, I would like to co-locate in-situ measurements and satellite data near the European coast for cross-validation of data or model validation, during the winter of 2019. (*Note: this user stroes was developed to demo at the 2nd Annual SWOT Applications Early Adopter Hackweek, 8 March 2021.)\n\n#### Learning Objectives\n- Co-locate remote sensing data from the Earthdata Cloud archive with in-situ measurements from another provider, programmatically using the Earthdata CMR amd Harmony APIs. \n- Workflow can be conducted either locally or in the cloud (i.e. it is compute environment agnostic). \n- While capabilities demoed here are shown through an oceanography example, these use cases and examples can be applied as building blocks for developing other user workflows with PO.DAAC and Earthdata datasets, across a range of science and applications disciplines, including for example terrestrial hydrology, coastal, or cryosphere. \n- Note: Searching for NASA Earthdata data given point-based observation locations can also be done via the NASA Earthdata Search user interface https://search.earthdata.nasa.gov/search, in addition to programmatically (as shown here).\n\n#### Datasets used\n- Argo floats https://argo.ucsd.edu/\n- MODIS-Aqua L2 SST https://podaac.jpl.nasa.gov/dataset/AMSRE-REMSS-L2P-v7a\n- MUR L4 SST https://registry.opendata.aws/mur/\n\n#### Main Steps in Workflow\n1. Define study region and period of time of interest: Atlantic Ocean west of Portugal and Morocco, January 2019\n2. Get in-situ Argo floats using the Argo API and prepapre the Argo data (select Argo SST for one float during its journey in Jan 2019 at the top pressure level i.e. nearest the ocean surface)\n3. Get coincident SST observed by the MODIS satellite, from the NASA Earthdata Cloud (in AWS)\n    - Search Earthdata Cloud satellite data for collection of interest (MODIS-Aqua L2) (using the CMR API)\n    - Extract satellite data at the in-situ location for direct comparison (using the Harmony API)\n    - Download locally (from the cloud archive), or download to your cloud storage or compute space if working within the AWS cloud\n    - Quality control the MODIS data with daytime and quality flag filters    \n4. Plot time series comparing the in-situ and satellite data at in-situ location(s)\n5. Validate with a third dataset, MUR L4 SST (once version stored in the *AWS Registry of Open Data* - public data access)\n\n*Note: in order to currently access PO.DAAC Cloud Pathfinder datasets such as MODIS SST L2 from the Earthdata Cloud, your Earthdata login username needs to be added to an restrcited early access list (during the transition period of migrating PO.DAAC data to the Earthdata Cloud). Please contact podaac@podaac.jpl.nasa.gov to make that request.*\n\n\n<img src=\"browse.png\" />\n\n---------------\n\n### Requirements\n\n**Import modules**: The Python ecosystem is organized into modules.  A module must be imported before the contents of that modules can be used.  It is good practice to import modules in the first code cell of a notebook or at the top of your script.  Not only does this make it clear which modules are being used, but it also ensures that the code fails at the beginning because one of the modules is not installed rather half way through after crunching a load of data.\n\nFor some modules, it is common practice to shorten the module names according to accepted conventions.  For example, the plotting module `matplotlib.pyplot` is shortened to `plt`.  It is best to stick to these conventions rather than making up your own short names so that people reading your code see immediately what you are doing. \n\n### Earthdata Login\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\nThe `setup_earthdata_login_auth` function will allow Python scripts to log into any Earthdata Login application programmatically.  To avoid being prompted for\ncredentials every time you run and also allow clients such as curl to log in, you can add the following\nto a `.netrc` (`_netrc` on Windows) file in your home directory:\n\n```\nmachine urs.earthdata.nasa.gov\n    login <your username>\n    password <your password>\n```\n\nMake sure that this file is only readable by the current user or you will receive an error stating\n\"netrc access too permissive.\"\n\n`$ chmod 0600 ~/.netrc` \n\n### Study region and period\n\nSet some \"master\" inputs to define the time and place contexts for our case studies in the ipynb. \n\nPlot the spatial extent of our study area with a blue polygon:\n\n## Access temperature profiles from ArgoVis API\n\nArgoVis is an API and visualization service that provides access to Argo float profiles. The endpoint for requesting profile data is given in the cell below:\n\nCreate the AOI polygon in required XY format, make it a string, and collect the dictionary of API parameters:\n\nSubmit the request parameters to the Argovis API. You should receive a JSON response back. Print the number of profiles inside our AOI:\n\n### Prepare profile data for further analysis\n\nConcatenate the list of metadata dictionaries returned for the argos into a table and update a few of its columns with Pythonic types:\n\nYou can download profiles in netCDF format from the FTP link stored in the *nc_url* fields of the response. Here's the URL for the first of the profiles:\n\nDisplay a table summarizing the space/time characteristics of eaach profile:\n\n*Now plot argo profile locations on an interactive map.*\n\nThis plot uses folium/leaflet. Hover/click the clusters (which correspond to specific Argo float platforms) to zoom to the groups of individual profiles and display metadata about them:\n\n#### Reformat profile data into data frames\n\nThe in situ measurements temperature, pressure, and salinity readings collected during each profile are returned inside the JSON response.\n\nThe format of the *measurements* field is perfect for conversion to *pandas* data frames. Apply *pandas.DataFrame* over the entire measurements column to make a pandas.Series of data frames, and replace the existing content in the *measurements* column:\n\n**Plot temperature at the minimum pressure for each profile**\n\nThis cell applies a lambda over the measurements column to slice the row corresponding to the minimum pressure bin for each profile and returns the corresponding temperature measurement:\n\n#### Select an *Argo of Interest* and its *platform_number*\n\nSee which floats had the most profiles within our timeframe/area of interest:\n\nChoose a float with six profiles to study further during the remainder of the notebook.\n\n## Access *sea surface temperature* from MODIS\n\nThe user guide for MODIS Level 2 Sea Surface Temperature (SST) from GHRSST is available on the PO.DAAC Drive: https://podaac-tools.jpl.nasa.gov/drive/files/OceanTemperature/ghrsst/docs/GDS20r5.pdf\n\nWe will access L2 SST data for our AOI and time period of interest by submitting two subset requests to the [Harmony API](https://harmony.earthdata.nasa.gov/).\n\n**Redefine the AOI to the minimum XY bounds of selected profiles**\n\nSimply replace the *`aoi_*`* Python variables with min/max of the *lat* and *lon* columns in the new *argo_skinny* data frame:\n\n**Search the Common Metadata Repository (CMR) for its unique *concept-id***\n\nThe API requires a dataset identifier that we must obtain from CMR. In the next cell, submit a request to the CMR API to grab the metadata for to the dataset/collection.\n\n#### Request subsets from the Harmony API\n\nWe will submit two requests to the [Harmony API](https://harmony.earthdata.nasa.gov/). The API is under active development, and it's therefore recommended that you test your input parameters in the [Swagger API interface](https://harmony.earthdata.nasa.gov/docs/api/#).\n\nThe next cell joins the base url for the API to the concept-id obtained above. Run the cell and print the complete url to confirm:\n\nMake a dictionary of subset parameters and format the values to meet requirements of the Harmony API. (See the Swagger UI linked above for more information about those requirements.)\n\n*Note how I've commented out the `time` parameter for the second half of January.* I requested the first 15 days and then the second 15 days in two requests to get the whole month. \n\nHere we print the parameters for the first request:\n\nComplete the url by formatting the query portion using the parameters dictionary:\n\n#### Submit the request parameters to the Harmony API endpoint\n\nI've already submitted the two requests required to obtain full coverage for our region and timeframe of interest (the two urls in the *job_status* list below). To submit a new request, or to submit these two MODIS requests again, comment out the two items in the list like this:\n\n```python\njob_status = [\n    #'https://...'\n    #'https://...\n]\n```\n\nIt should trigger new requests in the subsequent cells.\n\nThe next cell should download a JSON for your new request or from the first request that I submitted while I developed this notebook.\n\nPrint the message field of the JSON response:\n\nSuccessful requests to the API will respond with a JSON that starts like this:\n\n```json\n{\n  \"username\": \"jmcnelis\",\n  \"status\": \"running\",\n  \"message\": \"The job is being processed\",\n  \"progress\": 0,\n  \"createdAt\": \"2021-02-25T02:09:35.972Z\",\n  \"updatedAt\": \"2021-02-25T02:09:35.972Z\",\n    ...\n```\n\nThe example above is truncated to the first several lines for the sake of space.\n\n**Monitor the status of an in-progress job**\n\nSelect the `status` URL(s) from the list(s) of `links`:\n\nRun the next cell to monitor the status of as many requests as you need.\n\nIt will loop over the `job_status` list and wait for all the requests to finish processing. (It terminates when the `status` field of the JSON response _does not_ contain the string `\"running\"`.)\n\nThe final response(s) are *massive* whenever your subset results in a large number of output granules. Print everything but the `links` here:\n\nNow look at the first url that points to a subset file (skip the first two because they point to other stuff about the order):\n\nThis cell collects all the output links (Python dicts) from our requests in a list and prints the total number of outputs:\n\n##### Prepare subset data for further analysis\n\nGet the subset metadata as `pandas.DataFrame`. We can use *apply* logic to calculate stats over the time series in subsequent steps. Print the number of rows to confirm. (Should match above)\n\n**Select day/drop night observations**\n\nAdd a day/night flag column to the table. Apply a function over the `href` column to check the source filename for a string indicating day/night for the swath:\n\nAnd finally, reformat the *start* timestamps as a new column containing pandas datetime objects instead of strings. Then, add one more column containing a date object (rather than the full datetime timestamp) which we'll use to aggregate the data before plotting.\n\n### Accessing outputs from your subset request\n\nNow we will download all the netCDF subsets to the local workspace. (I'm inside AWS as I develop this ipynb.) Set a target directory and create it if needed:\n\nThis function should handle downloads reliably--test by downloading the first netCDF subset from our table (*subsets_df*):\n\nMake sure you can dump the header of that file with *ncdump*. (The output below is truncated.)\n\nNetCDF file format errors indicate that the download was not successful. *cat* the file for more clues. Read and plot the *sea_surface_temperature* variable:\n\n#### Download all the netCDF subsets\n\nGet the links in the *href* column in a loop:\n\nThe next cell adds a column of absolute paths to the netCDF files to the data frame *subsets_df*:\n\n#### Limit to daytime MODIS observations\n\nSelect just the *daytime* observations into a new data frame. (Remember we added a *daytime* column during a previous step.)\n\n#### Data quality\n\nThe *quality_level* variable describes the observation quality for each pixel in the L2 swaths. Values are assigned between 1 and 6 corresponding to these quality levels:\n\n1. no_data\n2. bad_data\n3. worst_quality\n4. low_quality\n5. acceptable_quality\n6. best_quality\n\nThe next cell plots the masked SST grid for the first daytime observations:\n\n### Plot time series from multiple data sources\n\nRoll the logic above into a few map-able functions that group the SST data by day to produce (up to) one daily mean.\n\n>#### Apply filter and mean in two functions\n>\n>`get_user_stat` reads the input netCDF and applies some user-specified function to the dataset to render the desired output, then closes the file.\n>\n>The second function `_masked_mean` filters and calculates the XY mean of the *sea_surface_temperature* variable. (You could replace this function with your own to do something different.)\n\nTest the combined routine against the first file in the daytime MODIS table:\n\nThat should give a reasonable value in degrees Celsius.\n\n##### Get means for the filtered MODIS SST time series in a new column\n\nApply the `_masked_mean` function over the column of subsets (i.e. netCDF4 files) to get the time series in a new column *sst_mean*:\n\nWe may need to group by the date:\n\nNow plot the two time series along the same *date* axis for visual comparison:\n\n### MUR Level 4 SST from AWS Open Registry\n\nTry plotting the summarized time series for the two datasets against MUR L4 SST from AWS Open Registry: https://registry.opendata.aws/mur/\n\nAdd the MUR time series to the subsets table so that they share the same time axis with the L2 time series:\n\nPlot the result alongside our data processed throughout the notebook:\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../include-files.lua","quarto"],"toc":true,"output-file":"Colocate_satellite_insitu_ocean.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":{"light":"cosmo","dark":"darkly"},"code-copy":true},"extensions":{"book":{"multiFile":true}}}}}